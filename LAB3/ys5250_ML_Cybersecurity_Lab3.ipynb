{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c370d316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1500/1500 [==============================] - 1s 605us/step - loss: 0.3694 - accuracy: 0.8888 - val_loss: 0.1457 - val_accuracy: 0.9583\n",
      "Epoch 2/10\n",
      "1500/1500 [==============================] - 1s 556us/step - loss: 0.1752 - accuracy: 0.9472 - val_loss: 0.1151 - val_accuracy: 0.9650\n",
      "Epoch 3/10\n",
      "1500/1500 [==============================] - 1s 559us/step - loss: 0.1357 - accuracy: 0.9599 - val_loss: 0.1048 - val_accuracy: 0.9678\n",
      "Epoch 4/10\n",
      "1500/1500 [==============================] - 1s 558us/step - loss: 0.1155 - accuracy: 0.9642 - val_loss: 0.0989 - val_accuracy: 0.9695\n",
      "Epoch 5/10\n",
      "1500/1500 [==============================] - 1s 565us/step - loss: 0.1016 - accuracy: 0.9696 - val_loss: 0.0877 - val_accuracy: 0.9737\n",
      "Epoch 6/10\n",
      "1500/1500 [==============================] - 1s 637us/step - loss: 0.0887 - accuracy: 0.9721 - val_loss: 0.0892 - val_accuracy: 0.9738\n",
      "Epoch 7/10\n",
      "1500/1500 [==============================] - 1s 560us/step - loss: 0.0804 - accuracy: 0.9735 - val_loss: 0.0875 - val_accuracy: 0.9758\n",
      "Epoch 8/10\n",
      "1500/1500 [==============================] - 1s 558us/step - loss: 0.0760 - accuracy: 0.9766 - val_loss: 0.0849 - val_accuracy: 0.9767\n",
      "Epoch 9/10\n",
      "1500/1500 [==============================] - 1s 574us/step - loss: 0.0682 - accuracy: 0.9785 - val_loss: 0.0823 - val_accuracy: 0.9768\n",
      "Epoch 10/10\n",
      "1500/1500 [==============================] - 1s 574us/step - loss: 0.0650 - accuracy: 0.9789 - val_loss: 0.0834 - val_accuracy: 0.9757\n",
      "313/313 [==============================] - 0s 305us/step - loss: 0.0876 - accuracy: 0.9760\n",
      "Clean Test Accuracy: 0.9760000109672546\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load MNIST dataset\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Normalize pixel values to [0, 1]\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
    "\n",
    "# Split data into training and validation sets\n",
    "train_images, val_images, train_labels, val_labels = train_test_split(train_images, train_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Deep neural network \n",
    "model = models.Sequential([\n",
    "    layers.Flatten(input_shape=(28, 28)), \n",
    "    layers.Dense(128, activation='relu'),   \n",
    "    layers.Dropout(0.2),                    \n",
    "    layers.Dense(64, activation='relu'),    \n",
    "    layers.Dropout(0.2),                    \n",
    "    layers.Dense(10, activation='softmax')  \n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model for 5 epochs\n",
    "history = model.fit(train_images, train_labels, epochs=10, validation_data=(val_images, val_labels))\n",
    "\n",
    "# Evaluate the model on clean test images\n",
    "test_loss, test_accuracy = model.evaluate(test_images, test_labels)\n",
    "print(f\"Clean Test Accuracy: {test_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60f9fbb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 271us/step\n",
      "313/313 [==============================] - 0s 262us/step\n",
      "For epsilon=0.01:\n",
      "Success Rate: 0.022438524590163933\n",
      "\n",
      "\n",
      "313/313 [==============================] - 0s 278us/step\n",
      "313/313 [==============================] - 0s 268us/step\n",
      "For epsilon=0.05:\n",
      "Success Rate: 0.270594262295082\n",
      "\n",
      "\n",
      "313/313 [==============================] - 0s 269us/step\n",
      "313/313 [==============================] - 0s 265us/step\n",
      "For epsilon=0.1:\n",
      "Success Rate: 0.7050204918032786\n",
      "\n",
      "\n",
      "313/313 [==============================] - 0s 277us/step\n",
      "313/313 [==============================] - 0s 265us/step\n",
      "For epsilon=0.15:\n",
      "Success Rate: 0.8498975409836066\n",
      "\n",
      "\n",
      "313/313 [==============================] - 0s 269us/step\n",
      "313/313 [==============================] - 0s 268us/step\n",
      "For epsilon=0.2:\n",
      "Success Rate: 0.9028688524590164\n",
      "\n",
      "\n",
      "313/313 [==============================] - 0s 271us/step\n",
      "313/313 [==============================] - 0s 268us/step\n",
      "For epsilon=0.4:\n",
      "Success Rate: 0.9665983606557377\n",
      "\n",
      "\n",
      "313/313 [==============================] - 0s 267us/step\n",
      "313/313 [==============================] - 0s 263us/step\n",
      "For epsilon=0.49019607843137253:\n",
      "Success Rate: 0.9738729508196722\n",
      "\n",
      "\n",
      "Overall Success Rate for epsilon=0.01: 0.022438524590163933\n",
      "Overall Success Rate for epsilon=0.05: 0.270594262295082\n",
      "Overall Success Rate for epsilon=0.1: 0.7050204918032786\n",
      "Overall Success Rate for epsilon=0.15: 0.8498975409836066\n",
      "Overall Success Rate for epsilon=0.2: 0.9028688524590164\n",
      "Overall Success Rate for epsilon=0.4: 0.9665983606557377\n",
      "Overall Success Rate for epsilon=0.49019607843137253: 0.9738729508196722\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def fgsm_attack(model, image, label, epsilon):\n",
    "    # Ensuring the image is of type float32\n",
    "    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
    "    \n",
    "    # Adding a batch dimension and channel dimension\n",
    "    image = tf.expand_dims(image, axis=0)\n",
    "    image = tf.expand_dims(image, axis=-1)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(image)\n",
    "        prediction = model(image)\n",
    "        loss = tf.keras.losses.sparse_categorical_crossentropy(label, prediction)\n",
    "    \n",
    "    gradient = tape.gradient(loss, image)\n",
    "    perturbation = epsilon * tf.sign(gradient)\n",
    "    adv_image = image + perturbation\n",
    "    \n",
    "    adv_image = tf.clip_by_value(adv_image, 0, 1)\n",
    "\n",
    "    # Removing the added dimensions after perturbation\n",
    "    adv_image = tf.squeeze(adv_image, axis=[0, -1])\n",
    "\n",
    "    return adv_image.numpy()\n",
    "\n",
    "\n",
    "def evaluate_attack(model, test_images, test_labels, adv_images):\n",
    "    # Evaluating the model on clean test images\n",
    "    clean_predictions = model.predict(test_images)\n",
    "    clean_labels = np.argmax(clean_predictions, axis=1)\n",
    "    \n",
    "    # Evaluating the model on adversarial images\n",
    "    adv_predictions = model.predict(adv_images)\n",
    "    adv_labels = np.argmax(adv_predictions, axis=1)\n",
    "     \n",
    "    clean_correct_mask = np.argmax(clean_predictions, axis=1) == test_labels\n",
    "    \n",
    "    # Counting the number of misclassified images after perturbation\n",
    "    misclassified_after_perturbation = np.sum(clean_correct_mask & (test_labels != adv_labels))\n",
    "    \n",
    "    # Counting the number of clean images correctly classified\n",
    "    clean_correct = np.sum(clean_correct_mask)\n",
    "    \n",
    "    # Calculating the success rate for targeted attack\n",
    "    success_rate = misclassified_after_perturbation / clean_correct\n",
    "\n",
    "    return success_rate\n",
    "\n",
    "epsilon_values = [0.01, 0.05, 0.1, 0.15, 0.2, 0.4, 125 / 255.0]\n",
    "success_rates = []\n",
    "\n",
    "for epsilon in epsilon_values:\n",
    "    adv_images = [fgsm_attack(model, img, label, epsilon) for img, label in zip(test_images, test_labels)]\n",
    "    success_rate = evaluate_attack(model, test_images, test_labels, np.array(adv_images))\n",
    "    \n",
    "    # Report results\n",
    "    print(f\"For epsilon={epsilon}:\")\n",
    "    print(f\"Success Rate: {success_rate}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "    success_rates.append(success_rate)\n",
    "\n",
    "for epsilon, success_rate in zip(epsilon_values, success_rates):\n",
    "    print(f\"Overall Success Rate for epsilon={epsilon}: {success_rate}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b8bb8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 270us/step\n",
      "313/313 [==============================] - 0s 269us/step\n",
      "For targeted epsilon=0.01:\n",
      "Success Rate (Targeted): 0.0025614754098360654\n",
      "\n",
      "\n",
      "313/313 [==============================] - 0s 271us/step\n",
      "313/313 [==============================] - 0s 264us/step\n",
      "For targeted epsilon=0.05:\n",
      "Success Rate (Targeted): 0.09139344262295082\n",
      "\n",
      "\n",
      "313/313 [==============================] - 0s 265us/step\n",
      "313/313 [==============================] - 0s 270us/step\n",
      "For targeted epsilon=0.1:\n",
      "Success Rate (Targeted): 0.2964139344262295\n",
      "\n",
      "\n",
      "313/313 [==============================] - 0s 268us/step\n",
      "313/313 [==============================] - 0s 269us/step\n",
      "For targeted epsilon=0.15:\n",
      "Success Rate (Targeted): 0.3793032786885246\n",
      "\n",
      "\n",
      "313/313 [==============================] - 0s 264us/step\n",
      "313/313 [==============================] - 0s 267us/step\n",
      "For targeted epsilon=0.2:\n",
      "Success Rate (Targeted): 0.4101434426229508\n",
      "\n",
      "\n",
      "313/313 [==============================] - 0s 277us/step\n",
      "313/313 [==============================] - 0s 266us/step\n",
      "For targeted epsilon=0.4:\n",
      "Success Rate (Targeted): 0.4255122950819672\n",
      "\n",
      "\n",
      "313/313 [==============================] - 0s 270us/step\n",
      "313/313 [==============================] - 0s 269us/step\n",
      "For targeted epsilon=0.49019607843137253:\n",
      "Success Rate (Targeted): 0.42704918032786887\n",
      "\n",
      "\n",
      "Overall Success Rate (Targeted) for epsilon=0.01: 0.0025614754098360654\n",
      "Overall Success Rate (Targeted) for epsilon=0.05: 0.09139344262295082\n",
      "Overall Success Rate (Targeted) for epsilon=0.1: 0.2964139344262295\n",
      "Overall Success Rate (Targeted) for epsilon=0.15: 0.3793032786885246\n",
      "Overall Success Rate (Targeted) for epsilon=0.2: 0.4101434426229508\n",
      "Overall Success Rate (Targeted) for epsilon=0.4: 0.4255122950819672\n",
      "Overall Success Rate (Targeted) for epsilon=0.49019607843137253: 0.42704918032786887\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def targeted_fgsm_attack(model, image, target_label, epsilon):\n",
    "    image = tf.convert_to_tensor(image, dtype=tf.float32)\n",
    "\n",
    "    # Adding a batch dimension and channel dimension\n",
    "    image = tf.expand_dims(image, axis=0)\n",
    "    image = tf.expand_dims(image, axis=-1)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(image)\n",
    "        prediction = model(image)\n",
    "        loss = tf.keras.losses.sparse_categorical_crossentropy(target_label, prediction)\n",
    "\n",
    "    gradient = tape.gradient(loss, image)\n",
    "    perturbation = -epsilon * tf.sign(gradient)\n",
    "    adv_image = image + perturbation\n",
    "    adv_image = tf.clip_by_value(adv_image, 0, 1)\n",
    "    \n",
    "    # Removing the added dimensions after perturbation\n",
    "    adv_image = tf.squeeze(adv_image, axis=[0, -1])\n",
    "\n",
    "    return adv_image.numpy()\n",
    "\n",
    "def evaluate_attack(model, test_images, test_labels, adv_images_targeted):\n",
    "    clean_predictions = model.predict(test_images)\n",
    "    clean_labels = np.argmax(clean_predictions, axis=1)\n",
    "    clean_correct_mask = np.argmax(clean_predictions, axis=1) == test_labels\n",
    "\n",
    "    adv_images_targeted_np = np.array(adv_images_targeted) \n",
    "\n",
    "    adv_predictions_targeted = model.predict(adv_images_targeted_np)\n",
    "    adv_labels_targeted = np.argmax(adv_predictions_targeted, axis=1)\n",
    "\n",
    "    misclassified_after_perturbation = np.sum(clean_correct_mask & (((test_labels + 1) % 10) == adv_labels_targeted))\n",
    "    clean_correct = np.sum(clean_correct_mask)\n",
    "    success_rate_targeted = misclassified_after_perturbation / clean_correct\n",
    "\n",
    "    return success_rate_targeted\n",
    "\n",
    "# Applying targeted FGSM attack on test images\n",
    "target_labels = [(label + 1) % 10 for label in test_labels]\n",
    "epsilon_values = [0.01, 0.05, 0.1, 0.15, 0.2, 0.4, 125 / 255.0]  \n",
    "success_rates_targeted = []\n",
    "\n",
    "for epsilon in epsilon_values:\n",
    "    adv_images_targeted = [targeted_fgsm_attack(model, img, label, epsilon)\n",
    "                           for img, label in zip(test_images, target_labels)]\n",
    "\n",
    "    success_rate_targeted = evaluate_attack(model, test_images, test_labels, adv_images_targeted)\n",
    "    success_rates_targeted.append(success_rate_targeted)\n",
    "\n",
    "    print(f\"For targeted epsilon={epsilon}:\")\n",
    "    print(f\"Success Rate (Targeted): {success_rate_targeted}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "for epsilon, success_rate_targeted in zip(epsilon_values, success_rates_targeted):\n",
    "    print(f\"Overall Success Rate (Targeted) for epsilon={epsilon}: {success_rate_targeted}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "710a20c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2400/2400 [==============================] - 3s 662us/step - loss: 0.2995 - accuracy: 0.9082 - val_loss: 0.1173 - val_accuracy: 0.9657\n",
      "Epoch 2/10\n",
      "2400/2400 [==============================] - 1s 594us/step - loss: 0.1338 - accuracy: 0.9595 - val_loss: 0.0831 - val_accuracy: 0.9745\n",
      "Epoch 3/10\n",
      "2400/2400 [==============================] - 1s 599us/step - loss: 0.1047 - accuracy: 0.9679 - val_loss: 0.0710 - val_accuracy: 0.9784\n",
      "Epoch 4/10\n",
      "2400/2400 [==============================] - 1s 588us/step - loss: 0.0897 - accuracy: 0.9724 - val_loss: 0.0671 - val_accuracy: 0.9815\n",
      "Epoch 5/10\n",
      "2400/2400 [==============================] - 1s 575us/step - loss: 0.0762 - accuracy: 0.9767 - val_loss: 0.0699 - val_accuracy: 0.9803\n",
      "Epoch 6/10\n",
      "2400/2400 [==============================] - 1s 612us/step - loss: 0.0698 - accuracy: 0.9789 - val_loss: 0.0628 - val_accuracy: 0.9819\n",
      "Epoch 7/10\n",
      "2400/2400 [==============================] - 1s 581us/step - loss: 0.0658 - accuracy: 0.9800 - val_loss: 0.0678 - val_accuracy: 0.9820\n",
      "Epoch 8/10\n",
      "2400/2400 [==============================] - 1s 575us/step - loss: 0.0585 - accuracy: 0.9824 - val_loss: 0.0604 - val_accuracy: 0.9836\n",
      "Epoch 9/10\n",
      "2400/2400 [==============================] - 1s 587us/step - loss: 0.0553 - accuracy: 0.9826 - val_loss: 0.0646 - val_accuracy: 0.9821\n",
      "Epoch 10/10\n",
      "2400/2400 [==============================] - 1s 587us/step - loss: 0.0526 - accuracy: 0.9836 - val_loss: 0.0596 - val_accuracy: 0.9835\n",
      "313/313 [==============================] - 0s 307us/step - loss: 0.0926 - accuracy: 0.9720\n",
      "Adversarially Retrained DNN Clean Test Accuracy: 0.972000002861023\n",
      "313/313 [==============================] - 0s 293us/step - loss: 0.1869 - accuracy: 0.9468\n",
      "313/313 [==============================] - 0s 268us/step\n",
      "313/313 [==============================] - 0s 270us/step\n",
      "For adversarially retrained epsilon=0.01:\n",
      "Adversarially Retrained DNN Test Accuracy: 0.9467999935150146\n",
      "Success Rate (Adversarially Retrained): 0.025925925925925925\n",
      "\n",
      "\n",
      "313/313 [==============================] - 0s 309us/step - loss: 1.4251 - accuracy: 0.6158\n",
      "313/313 [==============================] - 0s 267us/step\n",
      "313/313 [==============================] - 0s 265us/step\n",
      "For adversarially retrained epsilon=0.05:\n",
      "Adversarially Retrained DNN Test Accuracy: 0.6158000230789185\n",
      "Success Rate (Adversarially Retrained): 0.36646090534979425\n",
      "\n",
      "\n",
      "313/313 [==============================] - 0s 307us/step - loss: 4.4673 - accuracy: 0.1934\n",
      "313/313 [==============================] - 0s 262us/step\n",
      "313/313 [==============================] - 0s 269us/step\n",
      "For adversarially retrained epsilon=0.1:\n",
      "Adversarially Retrained DNN Test Accuracy: 0.19339999556541443\n",
      "Success Rate (Adversarially Retrained): 0.8010288065843622\n",
      "\n",
      "\n",
      "313/313 [==============================] - 0s 293us/step - loss: 7.3358 - accuracy: 0.0872\n",
      "313/313 [==============================] - 0s 269us/step\n",
      "313/313 [==============================] - 0s 266us/step\n",
      "For adversarially retrained epsilon=0.15:\n",
      "Adversarially Retrained DNN Test Accuracy: 0.08720000088214874\n",
      "Success Rate (Adversarially Retrained): 0.9102880658436214\n",
      "\n",
      "\n",
      "313/313 [==============================] - 0s 319us/step - loss: 10.3081 - accuracy: 0.0479\n",
      "313/313 [==============================] - 0s 288us/step\n",
      "313/313 [==============================] - 0s 358us/step\n",
      "For adversarially retrained epsilon=0.2:\n",
      "Adversarially Retrained DNN Test Accuracy: 0.0478999987244606\n",
      "Success Rate (Adversarially Retrained): 0.9507201646090535\n",
      "\n",
      "\n",
      "313/313 [==============================] - 0s 307us/step - loss: 25.1788 - accuracy: 0.0072\n",
      "313/313 [==============================] - 0s 271us/step\n",
      "313/313 [==============================] - 0s 273us/step\n",
      "For adversarially retrained epsilon=0.4:\n",
      "Adversarially Retrained DNN Test Accuracy: 0.007199999876320362\n",
      "Success Rate (Adversarially Retrained): 0.9925925925925926\n",
      "\n",
      "\n",
      "313/313 [==============================] - 0s 310us/step - loss: 33.0150 - accuracy: 0.0037\n",
      "313/313 [==============================] - 0s 277us/step\n",
      "313/313 [==============================] - 0s 279us/step\n",
      "For adversarially retrained epsilon=0.49019607843137253:\n",
      "Adversarially Retrained DNN Test Accuracy: 0.003700000001117587\n",
      "Success Rate (Adversarially Retrained): 0.9961934156378601\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define a function to create an adversarially retrained model\n",
    "def create_adversarially_retrained_model(original_model, adv_images, adv_labels):\n",
    "    # Append adversarially perturbed images and their correct labels to the training set\n",
    "    new_train_images = np.concatenate((train_images, adv_images))\n",
    "    new_train_labels = np.concatenate((train_labels, adv_labels))\n",
    "    \n",
    "    # Build and compile a new DNN model\n",
    "    retrained_model = models.clone_model(original_model)\n",
    "\n",
    "    # Split data into training and validation sets\n",
    "    train_images_adv, val_images_adv, train_labels_adv, val_labels_adv = train_test_split(new_train_images, new_train_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "    retrained_model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    # Train the new model\n",
    "    history = retrained_model.fit(train_images_adv, train_labels_adv, epochs=10, validation_data=(val_images_adv, val_labels_adv))\n",
    "    \n",
    "    return retrained_model\n",
    "\n",
    "\n",
    "# Apply FGSM attack on the training set for retraining\n",
    "epsilon_retrain = 125 / 255.0\n",
    "adv_images_retrain = [fgsm_attack(model, img, label, epsilon_retrain) for img, label in zip(train_images, train_labels)]\n",
    "\n",
    "# Create adversarially retrained model\n",
    "retrained_model = create_adversarially_retrained_model(model, np.array(adv_images_retrain), train_labels)\n",
    "\n",
    "# Evaluate the adversarially retrained model on clean test images\n",
    "retrained_test_loss, retrained_test_accuracy = retrained_model.evaluate(test_images, test_labels)\n",
    "print(f\"Adversarially Retrained DNN Clean Test Accuracy: {retrained_test_accuracy}\")\n",
    "\n",
    "# Evaluate the adversarially retrained model over a range of epsilon values\n",
    "epsilon_values = [0.01, 0.05, 0.1, 0.15, 0.2, 0.4, 125 / 255.0]\n",
    "for epsilon in epsilon_values:\n",
    "    # Apply FGSM attack on test images\n",
    "    adv_images_retrained = [fgsm_attack(retrained_model, img, label, epsilon) for img, label in zip(test_images, test_labels)]\n",
    "\n",
    "    # Evaluate the retrained model on adversarial images\n",
    "    retrained_adv_loss, retrained_adv_accuracy = retrained_model.evaluate(np.array(adv_images_retrained), test_labels)\n",
    "\n",
    "    # Calculate the success rate for adversarially retrained DNN\n",
    "    clean_predictions_retrained = retrained_model.predict(test_images)\n",
    "    clean_labels_retrained = np.argmax(clean_predictions_retrained, axis=1)\n",
    "    clean_correct_mask_retrained = clean_labels_retrained == test_labels\n",
    "\n",
    "    misclassified_after_perturbation_retrained = np.sum(clean_correct_mask_retrained & (clean_labels_retrained != np.argmax(retrained_model.predict(np.array(adv_images_retrained)), axis=1)))\n",
    "    success_rate_retrained = misclassified_after_perturbation_retrained / np.sum(clean_correct_mask_retrained)\n",
    "\n",
    "    print(f\"For adversarially retrained epsilon={epsilon}:\")\n",
    "    print(f\"Adversarially Retrained DNN Test Accuracy: {retrained_adv_accuracy}\")\n",
    "    print(f\"Success Rate (Adversarially Retrained): {success_rate_retrained}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f7d187",
   "metadata": {},
   "outputs": [],
   "source": [
    "We can see here that the adversarial training does not increase the robustness of the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
