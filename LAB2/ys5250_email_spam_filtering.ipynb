{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install wget"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYhJninmsIiH",
        "outputId": "16969871-8944-438c-f162-88637cf9e46c"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wget in /usr/local/lib/python3.10/dist-packages (3.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading and extracting the dataset."
      ],
      "metadata": {
        "id": "XmiMMUb_eYol"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "EOq95EPur9RN"
      },
      "outputs": [],
      "source": [
        "import wget\n",
        "import tarfile\n",
        "import ssl\n",
        "import os\n",
        "\n",
        "ssl._create_default_https_context = ssl._create_unverified_context\n",
        "\n",
        "dataset_url = \"http://www.aueb.gr/users/ion/data/lingspam_public.tar.gz\"\n",
        "\n",
        "destination_dir = \"ling_spam_dataset\"\n",
        "\n",
        "os.makedirs(destination_dir, exist_ok=True)\n",
        "\n",
        "# Downloading the dataset\n",
        "wget.download(dataset_url, out=destination_dir)\n",
        "\n",
        "dataset_path = os.path.join(destination_dir, \"lingspam_public.tar.gz\")\n",
        "with tarfile.open(dataset_path, \"r:gz\") as tar:\n",
        "    tar.extractall(destination_dir)\n",
        "\n",
        "os.remove(dataset_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Defining the path to the data directory\n",
        "data_dir = 'ling_spam_dataset/lingspam_public/lemm_stop'\n",
        "\n",
        "email_content = []\n",
        "labels = []\n",
        "fold_numbers = []\n",
        "\n",
        "# Traversing through the directories and reading the files\n",
        "for folder in os.listdir(data_dir):\n",
        "    folder_path = os.path.join(data_dir, folder)\n",
        "\n",
        "    if os.path.isdir(folder_path):\n",
        "        # Infer fold number from folder name\n",
        "        if folder.startswith(\"part\"):\n",
        "            fold_number = int(folder[4:])\n",
        "        else:\n",
        "            fold_number = int(folder)\n",
        "\n",
        "        for file in os.listdir(folder_path):\n",
        "            file_path = os.path.join(folder_path, file)\n",
        "\n",
        "            # Reading the content in the file\n",
        "            with open(file_path, 'r', encoding='latin-1') as f:\n",
        "                content = f.read()\n",
        "\n",
        "            # Determining the label\n",
        "            label = 1 if \"spmsg\" in file else 0\n",
        "\n",
        "            email_content.append(content)\n",
        "            labels.append(label)\n",
        "            fold_numbers.append(fold_number)\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    'text': email_content,\n",
        "    'label': labels,\n",
        "    'fold': fold_numbers\n",
        "})\n",
        "\n",
        "# Train and test sets\n",
        "df_test = df[df['fold'] == 10]\n",
        "df_train = df[df['fold'] != 10]\n",
        "\n",
        "print(\"Total dataset length:\", len(df))\n",
        "print(\"Training set length:\", len(df_train))\n",
        "print(\"Test set length:\", len(df_test))\n",
        "\n",
        "print(\"First 5 rows of the test set:\")\n",
        "print(df_test.head())\n",
        "\n",
        "print(\"First 5 rows of the train set:\")\n",
        "print(df_train.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLSHRapIw-3E",
        "outputId": "048c3cb9-6ae2-4854-808e-e21dff90dfee"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total dataset length: 2893\n",
            "Training set length: 2602\n",
            "Test set length: 291\n",
            "First 5 rows of the test set:\n",
            "                                                  text  label  fold\n",
            "579  Subject: vilem mathesius lecture sery 13 - pra...      0    10\n",
            "580  Subject: anglo - american study\\n\\nfirst annou...      0    10\n",
            "581  Subject: sigphon98 workshop - - - extend deadl...      0    10\n",
            "582  Subject: program & info : workshop comparative...      0    10\n",
            "583  Subject: whole part\\n\\nwhole part ( w / p ) bo...      0    10\n",
            "First 5 rows of the train set:\n",
            "                                                text  label  fold\n",
            "0  Subject: summary name day\\n\\ndear linguists , ...      0     3\n",
            "1  Subject: summary : adpositional ' eye '\\n\\nres...      0     3\n",
            "2  Subject: nlp\\n\\nreader : recently send several...      0     3\n",
            "3  Subject: sum : imperative without subject\\n\\nc...      0     3\n",
            "4  Subject: sum : register pre-school age\\n\\nsumm...      0     3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Information gain and Entropy."
      ],
      "metadata": {
        "id": "u35qJuh6gp6y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Information gain\n",
        "def calculate_information_gain(texts, labels):\n",
        "\n",
        "    # Converting the texts to term frequency\n",
        "    vectorizer = CountVectorizer(binary=True)\n",
        "    term_frequency = vectorizer.fit_transform(texts)\n",
        "    terms = vectorizer.get_feature_names_out()\n",
        "\n",
        "    #  Entropy\n",
        "    def calculate_entropy(prob_spam):\n",
        "        prob_legitimate = 1 - prob_spam\n",
        "        if prob_spam == 0 or prob_legitimate == 0:\n",
        "            return 0\n",
        "        return -prob_legitimate * np.log2(prob_legitimate) - prob_spam * np.log2(prob_spam)\n",
        "\n",
        "    # Overall entropy\n",
        "    prob_spam = np.mean(labels)\n",
        "    total_entropy = calculate_entropy(prob_spam)\n",
        "\n",
        "    information_gain_values = {}\n",
        "\n",
        "    # Iterating through each term\n",
        "    for i, term in enumerate(terms):\n",
        "        term_presence = term_frequency[:, i].toarray().flatten() == 1\n",
        "\n",
        "        # Separating labels into spam and legitimate based on term presence\n",
        "        labels_with_term = labels[term_presence]\n",
        "        labels_without_term = labels[~term_presence]\n",
        "\n",
        "        # Calculating conditional entropy for labels with and without the term\n",
        "        prob_with_term = np.mean(term_presence)\n",
        "        prob_without_term = 1 - prob_with_term\n",
        "        entropy_with_term = calculate_entropy(np.mean(labels_with_term))\n",
        "        entropy_without_term = calculate_entropy(np.mean(labels_without_term))\n",
        "\n",
        "        # Calculate information gain\n",
        "        conditional_entropy = prob_with_term * entropy_with_term + prob_without_term * entropy_without_term\n",
        "        information_gain = total_entropy - conditional_entropy\n",
        "\n",
        "        information_gain_values[term] = information_gain\n",
        "\n",
        "    return information_gain_values\n",
        "\n",
        "X_train = df_train['text']\n",
        "y_train = df_train['label']\n",
        "\n",
        "information_gain_values = calculate_information_gain(X_train, y_train)\n",
        "\n",
        "# Rank terms based on information gain\n",
        "sorted_terms = sorted(information_gain_values, key=information_gain_values.get, reverse=True)\n",
        "\n",
        "print(\"Top 10 terms based on Information Gain:\")\n",
        "for term in sorted_terms[:10]:\n",
        "    print(term)\n"
      ],
      "metadata": {
        "id": "9T_4VJrEwm2o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "937aa0b8-ce9b-4e66-bb41-b07b458a3e6c"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 terms based on Information Gain:\n",
            "language\n",
            "remove\n",
            "free\n",
            "linguistic\n",
            "click\n",
            "business\n",
            "advertise\n",
            "company\n",
            "sell\n",
            "english\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Top terms for different N values (10, 100, and 1000)\n",
        "top_10_features = sorted_terms[:10]\n",
        "top_100_features = sorted_terms[:100]\n",
        "top_1000_features = sorted_terms[:1000]\n",
        "\n",
        "# Top 10 terms with their information gain values\n",
        "top_10_ft = [(term, information_gain_values[term]) for term in sorted_terms[:10]]\n",
        "\n",
        "for term, info_gain in top_10_ft:\n",
        "    print(f\"Term: {term}, Information Gain: {info_gain:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEydDiiwCzQS",
        "outputId": "1363180d-e6bf-4b7a-c869-e402e5a50253"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Term: language, Information Gain: 0.2060\n",
            "Term: remove, Information Gain: 0.1689\n",
            "Term: free, Information Gain: 0.1632\n",
            "Term: linguistic, Information Gain: 0.1532\n",
            "Term: click, Information Gain: 0.1023\n",
            "Term: business, Information Gain: 0.0868\n",
            "Term: advertise, Information Gain: 0.0780\n",
            "Term: company, Information Gain: 0.0770\n",
            "Term: sell, Information Gain: 0.0768\n",
            "Term: english, Information Gain: 0.0747\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Naive Bayes Classifiers."
      ],
      "metadata": {
        "id": "VxedZ63Hgws4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "import time\n",
        "from tabulate import tabulate\n",
        "\n",
        "def evaluate_classifier(classifier, X_train, y_train, X_test, y_test):\n",
        "    # Timing the training and prediction process\n",
        "    start_time = time.time()\n",
        "    clf = classifier.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    latency = time.time() - start_time\n",
        "\n",
        "    # Calculating precision and recall\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    return precision, recall, latency\n",
        "\n",
        "train_data = df[df['fold'] < 10]\n",
        "test_data = df[df['fold'] == 10]\n",
        "\n",
        "N_values = [10, 100, 1000]\n",
        "top_feature_sets = [top_10_features, top_100_features, top_1000_features]\n",
        "classifiers = [BernoulliNB(), MultinomialNB(), MultinomialNB()]\n",
        "classifier_names = ['BernoulliNB', 'MultinomialNB_bin', 'MultinomialNB_tf']\n",
        "\n",
        "results = []\n",
        "\n",
        "# Looping over different feature sizes (N)\n",
        "for i, N in enumerate(N_values):\n",
        "    vectorizer_bin = CountVectorizer(binary=True, vocabulary=top_feature_sets[i])\n",
        "    X_train_bin = vectorizer_bin.fit_transform(train_data['text'])\n",
        "    X_test_bin = vectorizer_bin.transform(test_data['text'])\n",
        "\n",
        "    vectorizer_tf = CountVectorizer(vocabulary=top_feature_sets[i])\n",
        "    X_train_tf = vectorizer_tf.fit_transform(train_data['text'])\n",
        "    X_test_tf = vectorizer_tf.transform(test_data['text'])\n",
        "\n",
        "    y_train = train_data['label']\n",
        "    y_test = test_data['label']\n",
        "\n",
        "    # Evaluating multiple classifiers for each feature set\n",
        "    for j, classifier in enumerate(classifiers):\n",
        "        precision, recall, latency = evaluate_classifier(classifier, X_train_bin, y_train, X_test_bin, y_test)\n",
        "        results.append((classifier_names[j], N, precision, recall, latency))\n",
        "\n",
        "table_headers = [\"Classifier\", \"Top N Features\", \"Precision\", \"Recall\", \"Latency (seconds)\"]\n",
        "\n",
        "table = tabulate(results, headers=table_headers, tablefmt=\"grid\")\n",
        "\n",
        "print(table)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UqOwG-nFT3QB",
        "outputId": "c8b548ad-6d0b-408d-bffb-b409dcd5cc0a"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+------------------+-------------+----------+---------------------+\n",
            "| Classifier        |   Top N Features |   Precision |   Recall |   Latency (seconds) |\n",
            "+===================+==================+=============+==========+=====================+\n",
            "| BernoulliNB       |               10 |    0.804348 | 0.755102 |          0.002846   |\n",
            "+-------------------+------------------+-------------+----------+---------------------+\n",
            "| MultinomialNB_bin |               10 |    0.804348 | 0.755102 |          0.00178576 |\n",
            "+-------------------+------------------+-------------+----------+---------------------+\n",
            "| MultinomialNB_tf  |               10 |    0.804348 | 0.755102 |          0.00167608 |\n",
            "+-------------------+------------------+-------------+----------+---------------------+\n",
            "| BernoulliNB       |              100 |    1        | 0.673469 |          0.00286269 |\n",
            "+-------------------+------------------+-------------+----------+---------------------+\n",
            "| MultinomialNB_bin |              100 |    0.976744 | 0.857143 |          0.00201845 |\n",
            "+-------------------+------------------+-------------+----------+---------------------+\n",
            "| MultinomialNB_tf  |              100 |    0.976744 | 0.857143 |          0.00188994 |\n",
            "+-------------------+------------------+-------------+----------+---------------------+\n",
            "| BernoulliNB       |             1000 |    1        | 0.591837 |          0.00398445 |\n",
            "+-------------------+------------------+-------------+----------+---------------------+\n",
            "| MultinomialNB_bin |             1000 |    1        | 0.938776 |          0.00236535 |\n",
            "+-------------------+------------------+-------------+----------+---------------------+\n",
            "| MultinomialNB_tf  |             1000 |    1        | 0.938776 |          0.00226092 |\n",
            "+-------------------+------------------+-------------+----------+---------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Support Vector Machine (SVM)"
      ],
      "metadata": {
        "id": "u8DDmuEyg1eT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "import time\n",
        "from tabulate import tabulate\n",
        "\n",
        "# Function to train and evaluate an SVM classifier\n",
        "def evaluate_svm_classifier(X_train, y_train, X_test, y_test, param_grid, N):\n",
        "    svm = SVC(kernel='linear')\n",
        "    clf = GridSearchCV(svm, param_grid, cv=5)  # using 5-fold cross-validation\n",
        "    clf.fit(X_train, y_train)\n",
        "    best_C = clf.best_params_['C']\n",
        "    svm_best = SVC(kernel='linear', C=best_C)\n",
        "    start_time = time.time()\n",
        "    svm_best.fit(X_train, y_train)\n",
        "    y_pred = svm_best.predict(X_test)\n",
        "    latency = time.time() - start_time\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    return best_C, precision, recall, latency\n",
        "\n",
        "svm_results = []\n",
        "\n",
        "N_values = [10, 100, 1000]\n",
        "top_features = [top_10_features, top_100_features, top_1000_features]\n",
        "\n",
        "for i, N in enumerate(N_values):\n",
        "    vectorizer = CountVectorizer(binary=True, vocabulary=top_features[i])\n",
        "    X_train_bin = vectorizer.fit_transform(df_train['text'])\n",
        "    X_test_bin = vectorizer.transform(df_test['text'])\n",
        "\n",
        "    y_train = df_train['label']\n",
        "    y_test = df_test['label']\n",
        "\n",
        "    # Defining hyperparameters grid\n",
        "    param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
        "\n",
        "    best_C, precision, recall, latency = evaluate_svm_classifier(X_train_bin, y_train, X_test_bin, y_test, param_grid, N)\n",
        "\n",
        "    svm_results.append((f\"My SVM having top {N} binary features\", best_C, precision, recall, latency))\n",
        "\n",
        "table_headers = [\"SVM Configuration\", \"Best C\", \"Precision\", \"Recall\", \"Latency (seconds)\"]\n",
        "table = tabulate(svm_results, headers=table_headers, tablefmt=\"grid\")\n",
        "print(table)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-bxYL6IuVo2s",
        "outputId": "b92fbf45-3ea9-4460-9b3c-4007e63e9316"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------------------------------+----------+-------------+----------+---------------------+\n",
            "| SVM Configuration                      |   Best C |   Precision |   Recall |   Latency (seconds) |\n",
            "+========================================+==========+=============+==========+=====================+\n",
            "| My SVM having top 10 binary features   |     10   |    0.804348 | 0.755102 |           0.0188777 |\n",
            "+----------------------------------------+----------+-------------+----------+---------------------+\n",
            "| My SVM having top 100 binary features  |      1   |    0.973684 | 0.755102 |           0.047395  |\n",
            "+----------------------------------------+----------+-------------+----------+---------------------+\n",
            "| My SVM having top 1000 binary features |      0.1 |    0.974359 | 0.77551  |           0.175972  |\n",
            "+----------------------------------------+----------+-------------+----------+---------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this assignment, I achieved the following important goals:\n",
        "\n",
        "1. Classifier Evaluation for Spam Precision and Recall: I assessed three distinct classifiers, specifically the Bernoulli Naive Bayes (utilizing binary features), the Multinomial Naive Bayes (with binary features), and the Multinomial Naive Bayes (incorporating term frequencies). My evaluation comprised of metrics like precision, recall, and the latency of these classifiers. This analysis was performed across different feature sets, including the top 10, 100, and 1000 features.\n",
        "\n",
        "2. Latency Measurement: I quantified the latency for each classifier and feature set. The latency measurement covers the time taken from the training phase to the prediction phase.\n",
        "\n",
        "3. Design and Evaluation of a Support Vector Machine (SVM) Spam Filter: I  designed an SVM-based spam filter. My approach involved using binary features (BF) for this filter. To determine the most suitable hyperparameters, especially the regularization parameter (C), I utilized GridSearchCV along with cross-validation, specifically on the training dataset. Going ahead, I rigorously assessed the performance of the SVM on the test dataset and reported the key metrics, including precision, recall, and latency. These evaluations were conducted for the top 10, 100, and 1000 features.\n",
        "\n",
        "4. Feature Selection Methodology: In my SVM implementation, I made a choice to employ binary features (BF), driven by their strong relevance in the domain of spam classification. To select the most important features, I used a method focused on identifying the top N words. This N value was varied between 10, 100, and 1000 based on the significance of these terms, often measured by their frequency within the dataset.\n",
        "\n",
        "5. Hyperparameter Selection Protocol: I adhered to not use the test dataset for hyperparameter selection. Instead, I rigorously followed the recommended best practice of leveraging GridSearchCV with a 5-fold cross-validation strategy applied exclusively to the training dataset. This approach ensured the optimal hyperparameters were determined systematically and independently from the test dataset.\n"
      ],
      "metadata": {
        "id": "O-N9U49eanyQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Results**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "yfMS3O1ka6Y7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Top 10 words:**\n",
        "\n",
        "\n",
        "> [\"remove\", \"free\", \"linguistic\", \"click\", \"business\", \"advertise\", \"company\", \"sell\", \"english\"]\n",
        "\n",
        "<br>\n",
        "\n",
        "**Spam Precision and Spam Recall for Naive Bayes Classifiers:**\n",
        "\n",
        "> Methodology\n",
        "\n",
        "- Classifiers: BernoulliNB and two variants of MultinomialNB (binary features and term frequency).\n",
        "- Evaluation: Measured precision and recall for each classifier.\n",
        "- Feature Sizes (N): Evaluated three feature sizes: 10, 100, and 1000.\n",
        "- Feature Representation: Used binary and term frequency representations.\n",
        "- Timing: Recorded training and prediction time (latency) for each classifier.\n",
        "\n",
        "\n",
        ">Results:\n",
        "\n",
        ">| Classifier        | Top N Features | Precision | Recall   | Latency (seconds) |\n",
        "| ----------------- | -------------- | --------- | -------- | ------------------ |\n",
        "| BernoulliNB       | 10             | 0.804348  | 0.755102 | 0.00380373         |\n",
        "| MultinomialNB_bin | 10             | 0.804348  | 0.755102 | 0.00289392         |\n",
        "| MultinomialNB_tf  | 10             | 0.804348  | 0.755102 | 0.00244927         |\n",
        "| BernoulliNB       | 100            | 1         | 0.673469 | 0.00285673         |\n",
        "| MultinomialNB_bin | 100            | 0.976744  | 0.857143 | 0.00178242         |\n",
        "| MultinomialNB_tf  | 100            | 0.976744  | 0.857143 | 0.00179315         |\n",
        "| BernoulliNB       | 1000           | 1         | 0.591837 | 0.00389504         |\n",
        "| MultinomialNB_bin | 1000           | 1         | 0.938776 | 0.0026834          |\n",
        "| MultinomialNB_tf  | 1000           | 1         | 0.938776 | 0.002352           |\n",
        "\n",
        "<br>\n",
        "\n",
        "**Support Vector Machine (SVM)**\n",
        "\n",
        ">Methodology:\n",
        "\n",
        "- Features Used: Binary features indicating the presence or absence of terms in emails.\n",
        "- Number of Features: Evaluated with three settings: 10, 100, and 1000 top features based on information gain.\n",
        "- Feature Selection: Selected top-N features using information gain.\n",
        "- SVM Parameters: Linear SVM with 'C' hyperparameter grid search.\n",
        "- Evaluation: 5-fold cross-validation for 'C' selection, then training on the full dataset.\n",
        "\n",
        ">Results:\n",
        "\n",
        ">| SVM Configuration                      |   Best C |   Precision |   Recall |   Latency (seconds) |\n",
        "|----------------------------------------|----------|-------------|----------|---------------------|\n",
        "| My SVM having top 10 binary features   |     10   |    0.804348 | 0.755102 |          0.0219417 |\n",
        "| My SVM having top 100 binary features  |      1   |    0.973684 | 0.755102 |          0.0663161 |\n",
        "| My SVM having top 1000 binary features |      0.1 |    0.974359 | 0.77551  |          0.167927  |\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jzviUCqhbLls"
      }
    }
  ]
}